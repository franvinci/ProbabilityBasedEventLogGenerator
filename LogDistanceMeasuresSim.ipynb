{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.eventlog_utils import convert_log_from_lc_to_se\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "from log_distance_measures.config import EventLogIDs, AbsoluteTimestampType, discretize_to_hour\n",
    "from log_distance_measures.control_flow_log_distance import control_flow_log_distance\n",
    "from log_distance_measures.n_gram_distribution import n_gram_distribution_distance\n",
    "from log_distance_measures.absolute_event_distribution import absolute_event_distribution_distance\n",
    "from log_distance_measures.case_arrival_distribution import case_arrival_distribution_distance\n",
    "from log_distance_measures.circadian_event_distribution import circadian_event_distribution_distance\n",
    "from log_distance_measures.relative_event_distribution import relative_event_distribution_distance\n",
    "from log_distance_measures.work_in_progress import work_in_progress_distance\n",
    "from log_distance_measures.cycle_time_distribution import cycle_time_distribution_distance\n",
    "from log_distance_measures import earth_movers_distance\n",
    "from src.res_based_ced import resource_based_circadian_event_distribution_distance\n",
    "from distance_utils import emd_attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_studies = {\n",
    "    1: 'Purchasing',\n",
    "    2: 'Production',\n",
    "    3: 'Consulta',\n",
    "    4: 'bpi12',\n",
    "    5: 'bpi17',\n",
    "    6: 'sepsis',\n",
    "    8: 'bpi19',\n",
    "    7: 'rtf'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case study: bpi17, our approach = False\n"
     ]
    }
   ],
   "source": [
    "# chose case study\n",
    "case_study = case_studies[5]\n",
    "\n",
    "# Choose if our approach or Sota\n",
    "our_approach = False\n",
    "Sota = not(our_approach)\n",
    "approach = 'SIMOD' if Sota else None\n",
    "print(f'Case study: {case_study}, our approach = {our_approach}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 5771/5771 [00:02<00:00, 2512.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported log from Sota, approach: SIMOD\n",
      "imported log from Sota, approach: SIMOD ,columns renamed\n"
     ]
    }
   ],
   "source": [
    "log_real = xes_importer.apply(f'data/{case_study}/logTest.xes')\n",
    "log_real = pm4py.convert_to_dataframe(log_real)\n",
    "\n",
    "if our_approach:\n",
    "    log_sim = pd.read_csv(f'simulations/{case_study}/sim_0.csv')\n",
    "    print('imported log from our approach')\n",
    "\n",
    "elif Sota:\n",
    "    \n",
    "    if approach == 'RIMS':\n",
    "        log_sim = pd.read_csv(f'RIMS/{case_study}/results/rims/sim.csv')\n",
    "        del log_sim['st_wip']\n",
    "        del log_sim['queue']\n",
    "        del log_sim['st_tsk_wip']\n",
    "        print('imported log from Sota, approach:', approach)        \n",
    "    \n",
    "    elif approach == 'AgentSimulator':\n",
    "        log_sim = pd.read_csv(f'AgentSimulator/{case_study}/main_results/sim.csv')\n",
    "        print('imported log from Sota, approach:', approach)    \n",
    "\n",
    "    elif approach == 'SIMOD':\n",
    "        log_sim = pd.read_csv(f'SIMOD/{case_study}/simulated_log_0.csv')\n",
    "        print('imported log from Sota, approach:', approach)\n",
    "\n",
    "    elif approach == 'DSIM':\n",
    "        log_sim = pd.read_csv(f'DSIM/results/generated_logs/{case_study}/DSIM/sim.csv')\n",
    "        print('imported log from Sota, approach:', approach)\n",
    "\n",
    "    else:\n",
    "        log_sim = pd.read_csv(f'Sota/{case_study}/{approach}/sim.csv')\n",
    "    \n",
    "    #Rename the columns named 'end_timestamp' and 'start_timestamp' with 'time:timestamp' and 'start:timestamp'\n",
    "    log_sim = log_sim.rename(columns={'end_timestamp': 'time:timestamp', \n",
    "                                      'start_timestamp': 'start:timestamp', \n",
    "                                      'task':'concept:name',\n",
    "                                      'caseid': 'case:concept:name',\n",
    "                                      'resource': 'org:resource'}, errors='ignore')\n",
    "    \n",
    "    log_sim = log_sim.rename(columns={'end_time': 'time:timestamp', \n",
    "                                        'start_time': 'start:timestamp', \n",
    "                                        'activity':'concept:name',\n",
    "                                        'case_id': 'case:concept:name',\n",
    "                                        'resource': 'org:resource'}, errors='ignore')\n",
    "    \n",
    "    # Errors='ignore' has been set because the log from francesca meneghello et al has some columns already correctly\n",
    "    print('imported log from Sota, approach:', approach, ',columns renamed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5771/5771 [01:28<00:00, 65.30it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert event log format lifecycles\n",
    "if 'lifecycle:transition' in log_real.columns:\n",
    "    log_real = convert_log_from_lc_to_se(log_real)\n",
    "    log_real.rename(columns={'START': 'start:timestamp', 'END': 'time:timestamp'}, errors='ignore', inplace=True)\n",
    "    log_real.reset_index(inplace=True)\n",
    "if 'lifecycle:transition' in log_sim.columns:\n",
    "    log_sim = convert_log_from_lc_to_se(log_sim)\n",
    "    log_sim.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set event log column ID mapping\n",
    "event_log_ids = EventLogIDs(\n",
    "    case=\"case:concept:name\",\n",
    "    activity=\"concept:name\",\n",
    "    start_time=\"start:timestamp\",\n",
    "    end_time=\"time:timestamp\",\n",
    "    resource=\"org:resource\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_real[event_log_ids.start_time] = pd.to_datetime(log_real[event_log_ids.start_time], utc=True)\n",
    "log_real[event_log_ids.end_time] = pd.to_datetime(log_real[event_log_ids.end_time], utc=True)\n",
    "\n",
    "\n",
    "# Convert timestamps strings to datetime objects\n",
    "log_sim[event_log_ids.start_time] = pd.to_datetime(log_sim[event_log_ids.start_time], utc=True)\n",
    "log_sim[event_log_ids.end_time] = pd.to_datetime(log_sim[event_log_ids.end_time], utc=True)\n",
    "\n",
    "# log_sim[event_log_ids.start_time] = [i[:19] for i in log_sim[event_log_ids.start_time].values]\n",
    "# log_sim[event_log_ids.end_time] = [i[:19] for i in log_sim[event_log_ids.end_time].values]\n",
    "# log_sim[event_log_ids.start_time] = pd.to_datetime(log_sim[event_log_ids.start_time], utc=True)\n",
    "# log_sim[event_log_ids.end_time] = pd.to_datetime(log_sim[event_log_ids.end_time], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize distances dictionary\n",
    "distances = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case study is:  bpi17 CWD is:  1.9388987368576223\n"
     ]
    }
   ],
   "source": [
    "from log_distance_measures.circadian_workforce_distribution import circadian_workforce_distribution_distance\n",
    "print('case study is: ', case_study, 'CWD is: ', circadian_workforce_distribution_distance(log_real, event_log_ids, log_sim, event_log_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generated Attributes Distribution Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr distance:  nan\n"
     ]
    }
   ],
   "source": [
    "distance = emd_attributes(log_real, log_sim, attr_names=[])\n",
    "print('Attr distance: ', distance)\n",
    "# Fill the distances dictionary\n",
    "distances['emd_attributes'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>WARNING: It may take a long time</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control-flow Log Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call passing the event logs, and its column ID mappings\n",
    "distance = control_flow_log_distance(\n",
    "    log_real, event_log_ids,  # First event log and its column id mappings\n",
    "    log_sim, event_log_ids,  # Second event log and its column id mappings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF Log distance:  0.5878994838548787\n"
     ]
    }
   ],
   "source": [
    "print('CF Log distance: ', distance)\n",
    "# Fill the distances dictionary\n",
    "distances['control_flow_log_distance'] = distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Distribution Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call passing the event logs, and its column ID mappings\n",
    "distance = n_gram_distribution_distance(\n",
    "    log_real, event_log_ids,  # First event log and its column id mappings\n",
    "    log_sim, event_log_ids,  # Second event log and its column id mappings\n",
    "    n=n_gram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-Gram distr. distance:  0.9762775141946022\n"
     ]
    }
   ],
   "source": [
    "print('N-Gram distr. distance: ', distance)\n",
    "# Fill the distances dictionary\n",
    "distances['n_gram_distribution_distance'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Absolute Event Distribution Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMD of the (END) timestamps distribution where each bin represents a minute\n",
    "distance = absolute_event_distribution_distance(\n",
    "    log_real, event_log_ids,\n",
    "    log_sim, event_log_ids,\n",
    "    discretize_type=AbsoluteTimestampType.END,\n",
    "    discretize_event=discretize_to_hour\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute Event Distribution Distance:  79222.95631580324\n"
     ]
    }
   ],
   "source": [
    "print('Absolute Event Distribution Distance: ', distance)\n",
    "# Fill the distances dictionary\n",
    "distances['absolute_event_distribution_distance'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Arrival Distribution Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = case_arrival_distribution_distance(\n",
    "    log_real, event_log_ids,  # First event log and its column id mappings\n",
    "    log_sim, event_log_ids,  # Second event log and its column id mappings\n",
    "    discretize_event=discretize_to_hour  # Function to discretize each timestamp (default by hour)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Arrival distr distance:  227.30366492146598\n"
     ]
    }
   ],
   "source": [
    "print('Case Arrival distr distance: ', distance)\n",
    "# Fill the distances dictionary\n",
    "distances['case_arrival_distribution_distance'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circadian Event Distribution Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = circadian_event_distribution_distance(\n",
    "    log_real, event_log_ids,  # First event log and its column id mappings\n",
    "    log_sim, event_log_ids,  # Second event log and its column id mappings\n",
    "    discretize_type=AbsoluteTimestampType.BOTH  # Consider both start/end timestamps of each activity instance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Circadian Event distr distance:  4.130469247980372\n"
     ]
    }
   ],
   "source": [
    "print('Circadian Event distr distance: ', distance)\n",
    "# Fill the distances dictionary \n",
    "distances['circadian_event_distribution_distance'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resource-Based Circadian Event Distribution Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = resource_based_circadian_event_distribution_distance(\n",
    "    log_real, event_log_ids,  # First event log and its column id mappings\n",
    "    log_sim, event_log_ids,  # Second event log and its column id mappings\n",
    "    discretize_type=AbsoluteTimestampType.BOTH  # Consider both start/end timestamps of each activity instance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resource-Based Circadian Event distr distance:  8.168591247366653\n"
     ]
    }
   ],
   "source": [
    "print('Resource-Based Circadian Event distr distance: ', distance)\n",
    "# Fill the distances dictionary \n",
    "distances['resource_based_circadian_event_distribution_distance'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative Event Distribution Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call passing the event logs, its column ID mappings, timestamp type, and discretize function\n",
    "distance = relative_event_distribution_distance(\n",
    "    log_real, event_log_ids,  # First event log and its column id mappings\n",
    "    log_sim, event_log_ids,  # Second event log and its column id mappings\n",
    "    discretize_type=AbsoluteTimestampType.BOTH,  # Type of timestamp distribution (consider start times and/or end times)\n",
    "    discretize_event=discretize_to_hour  # Function to discretize the absolute seconds of each timestamp (default by hour)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative Event distr distance:  79435.52090004024\n"
     ]
    }
   ],
   "source": [
    "print('Relative Event distr distance: ', distance)\n",
    "# Fill the distances dictionary\n",
    "distances['relative_event_distribution_distance'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work in Progress Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>WARNING: It may take a long time</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Call passing the event logs, its column ID mappings, timestamp type, and discretize function\n",
    "# distance = work_in_progress_distance(\n",
    "#     log_real, event_log_ids,  # First event log and its column id mappings\n",
    "#     log_sim, event_log_ids,  # Second event log and its column id mappings\n",
    "#     window_size=pd.Timedelta(hours=1)  # Bins of 1 hour\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Work in Progress distance: ', distance)\n",
    "# Fill the distances dictionary\n",
    "# distances['work_in_progress_distance'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cycle Time Distribution Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = cycle_time_distribution_distance(\n",
    "    log_real, event_log_ids,  # First event log and its column id mappings\n",
    "    log_sim, event_log_ids,  # Second event log and its column id mappings\n",
    "    bin_size=pd.Timedelta(hours=1)  # Bins of 1 minute\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cycle Time distr distance:  62797.13089005235\n"
     ]
    }
   ],
   "source": [
    "print('Cycle Time distr distance: ', distance)\n",
    "# Fill the distances dictionary\n",
    "distances['cycle_time_distribution_distance'] = distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case study is Consulta for approach SIMOD, with our approach False and Sota True\n",
      "emd_attributes nan\n",
      "control_flow_log_distance 0.5878994838548787\n",
      "n_gram_distribution_distance 0.9762775141946022\n",
      "absolute_event_distribution_distance 79222.95631580324\n",
      "case_arrival_distribution_distance 227.30366492146598\n",
      "circadian_event_distribution_distance 4.130469247980372\n",
      "resource_based_circadian_event_distribution_distance 8.168591247366653\n",
      "relative_event_distribution_distance 79435.52090004024\n",
      "cycle_time_distribution_distance 62797.13089005235\n"
     ]
    }
   ],
   "source": [
    "print(f'case study is {case_study} for approach {approach}, with our approach {our_approach} and Sota {Sota}')\n",
    "# Save the distances dictionary to a file with pkl\n",
    "pd.to_pickle(distances, f'simulations/{case_study}/metrics.pkl')\n",
    "\n",
    "# Print the distances vector with each distance measure, in different lines\n",
    "for key, value in distances.items():\n",
    "    print(key, value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cast them into .xes for evaluating entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import glob\n",
    "from src.eventlog_utils import convert_log_from_lc_to_se\n",
    "import pm4py\n",
    "from pm4py.objects.log.importer.xes import importer as xes_importer\n",
    "directory = '/home/padela/Scrivania/ProbabilityBasedEventLogGenerator/data'\n",
    "os.chdir('data')\n",
    "\n",
    "\n",
    "print('Folder s in directory:', directory)\n",
    "for folder in glob.glob('*'):  # Skip the first folder which is the current directory\n",
    "    if os.path.isdir(folder):\n",
    "        print(folder)\n",
    "        for file in glob.glob(f'{folder}/*.xes'):\n",
    "            log = xes_importer.apply(file)\n",
    "            log = pm4py.convert_to_dataframe(log)\n",
    "            if 'lifecycle:transition' in log.columns:\n",
    "                log = convert_log_from_lc_to_se(log)\n",
    "                log.rename(columns={'START': 'start:timestamp', 'END': 'time:timestamp'}, errors='ignore', inplace=True)\n",
    "                log.reset_index(inplace=True)\n",
    "            # Save the log to a csv file\n",
    "            log.to_csv(f'{folder}/{os.path.basename(file).replace(\".xes\", \".csv\")}', index=False)\n",
    "            print(f'Saved {file} to {folder}/{os.path.basename(file).replace(\".xes\", \".csv\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running case study: Purchasing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/bin/simod\", line 5, in <module>\n",
      "    from simod.cli import main\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/simod/cli.py\", line 12, in <module>\n",
      "    from simod.simod import Simod\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/simod/simod.py\", line 18, in <module>\n",
      "    from simod.branch_rules.discovery import discover_branch_rules, map_branch_rules_to_flows\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/simod/branch_rules/discovery.py\", line 8, in <module>\n",
      "    from pix_framework.discovery.gateway_conditions.gateway_conditions import discover_gateway_conditions\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/pix_framework/discovery/gateway_conditions/gateway_conditions.py\", line 8, in <module>\n",
      "    from pix_framework.discovery.gateway_conditions.helpers import log_time\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/pix_framework/discovery/gateway_conditions/helpers.py\", line 4, in <module>\n",
      "    from sklearn.tree import _tree\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/sklearn/__init__.py\", line 84, in <module>\n",
      "    from .base import clone\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/sklearn/base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/sklearn/utils/__init__.py\", line 9, in <module>\n",
      "    from . import _joblib, metadata_routing\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/sklearn/utils/_joblib.py\", line 9, in <module>\n",
      "    import joblib\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/joblib/__init__.py\", line 129, in <module>\n",
      "    from .parallel import Parallel\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/joblib/parallel.py\", line 31, in <module>\n",
      "    from ._parallel_backends import (FallbackToBackend, MultiprocessingBackend,\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/joblib/_parallel_backends.py\", line 12, in <module>\n",
      "    from ._utils import (\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/joblib/_utils.py\", line 11, in <module>\n",
      "    from .externals.loky.process_executor import _ExceptionWithTraceback\n",
      "  File \"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/site-packages/joblib/externals/loky/__init__.py\", line 17, in <module>\n",
      "    from ._base import Future\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1138, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 1078, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1507, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1479, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1619, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1658, in _fill_cache\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mCalledProcessError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcase_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/subprocess.py:1201\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1199\u001b[0m         stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1203\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/subprocess.py:1264\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1262\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m _time() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[1;32m   1268\u001b[0m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[1;32m   1269\u001b[0m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/subprocess.py:2053\u001b[0m, in \u001b[0;36mPopen._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[0;32m-> 2053\u001b[0m (pid, sts) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pid \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid:\n",
      "File \u001b[0;32m~/Desktop/DataAug/ProbabilityBasedEventLogGenerator/.conda/lib/python3.11/subprocess.py:2011\u001b[0m, in \u001b[0;36mPopen._try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[1;32m   2010\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2011\u001b[0m     (pid, sts) \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mwaitpid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid, wait_flags)\n\u001b[1;32m   2012\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[1;32m   2013\u001b[0m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[1;32m   2014\u001b[0m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[1;32m   2015\u001b[0m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[1;32m   2016\u001b[0m     pid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpid\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import subprocess\n",
    "\n",
    "case_studies = {\n",
    "    1: 'Purchasing',\n",
    "    2: 'Production',\n",
    "    3: 'Consulta',\n",
    "    4: 'bpi12',\n",
    "    5: 'bpi17',\n",
    "    6: 'sepsis',\n",
    "    7: 'rtf',\n",
    "    8: 'bpi19'\n",
    "}\n",
    "\n",
    "base_path = \"/home/padela/Scrivania/ProbabilityBasedEventLogGenerator/simod_new\"\n",
    "\n",
    "for key, case_name in case_studies.items():\n",
    "    config_path = f\"{base_path}/{case_name}.yml\"\n",
    "    command = [\"simod\", \"--configuration\", config_path]\n",
    "    \n",
    "    print(f\"\\nRunning case study: {case_name}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while running {case_name}: {e}\")\n",
    "    end_time = time.time()\n",
    "    \n",
    "    duration = end_time - start_time\n",
    "    print(f\"Execution time for {case_name}: {duration:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purchasing Time Cycle:  2.029741090810826\n",
      "Purchasing Time Act:  0.7476869516284058\n",
      "\n",
      "\n",
      "Consulta Time Cycle:  1.0876320807353892\n",
      "Consulta Time Act:  0.619089921860491\n",
      "\n",
      "\n",
      "sepsis Time Cycle:  2.129605053558443\n",
      "sepsis Time Act:  0.0\n",
      "\n",
      "\n",
      "Error processing bpi19: [Errno 2] No such file or directory: 'bpi19/simulated_log_0.csv'\n",
      "\n",
      "\n",
      "Error processing Production: [Errno 2] No such file or directory: 'Production/simulated_log_0.csv'\n",
      "\n",
      "\n",
      "bpi12 Time Cycle:  2.900298553067816\n",
      "bpi12 Time Act:  1.7172184450619847\n",
      "\n",
      "\n",
      "bpi17 Time Cycle:  3.3953641762340796\n",
      "bpi17 Time Act:  1.9501820935452605\n",
      "\n",
      "\n",
      "Error processing rtf: [Errno 2] No such file or directory: 'rtf/simulated_log_0.csv'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.entropies import cf_entropy, compute_etd_entropy, compute_ctd_entropy\n",
    "from src.eventlog_utils import convert_log_from_lc_to_se\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "approach = 'SIMOD'\n",
    "main_dir = f\"/home/padela/Desktop/DataAug/ProbabilityBasedEventLogGenerator/{approach}\"\n",
    "\n",
    "def separate_lifecycle_transition(df):\n",
    "    \"\"\"\n",
    "    Moves the lifecycle transition (e.g., 'start', 'complete') from the end of concept:name\n",
    "    to the lifecycle:transition column, and updates concept:name to remove the suffix.\n",
    "    \"\"\"\n",
    "    # Extract lifecycle transition from concept:name\n",
    "    extracted = df['concept:name'].str.extract(r'^(.*)_(start|complete)$')\n",
    "    \n",
    "    # If extraction was successful, update the columns\n",
    "    df.loc[extracted[1].notnull(), 'concept:name'] = extracted[0]\n",
    "    df.loc[extracted[1].notnull(), 'lifecycle:transition'] = extracted[1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "import os\n",
    "os.chdir(main_dir)\n",
    "\n",
    "import glob\n",
    "for folder in glob.glob('*'):\n",
    "    # if folder=='bpi12':\n",
    "    #     print('db')\n",
    "    try:\n",
    "        log = pd.read_csv(f'{folder}/simulated_log_0.csv', sep=',')\n",
    "        # log = separate_lifecycle_transition(log)\n",
    "        # log = convert_log_from_lc_to_se(log)\n",
    "        log = log.rename(columns={'end_time': 'time:timestamp', \n",
    "                                    'start_time': 'start:timestamp', \n",
    "                                    'activity':'concept:name',\n",
    "                                    'case_id': 'case:concept:name',\n",
    "                                    'resource': 'org:resource'}, errors='ignore')\n",
    "        \n",
    "        # print(f'{folder} Trace: ', cf_entropy(log, prefix=False))\n",
    "        # print(f'{folder} Act: ', cf_entropy(log, prefix=True))\n",
    "        print(f'{folder} Time Cycle: ', compute_ctd_entropy(log))\n",
    "        print(f'{folder} Time Act: ', compute_etd_entropy(log))\n",
    "\n",
    "        print('\\n')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing {folder}: {e}')\n",
    "        print('\\n')\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
